



<!-- PROJECT LOGO -->
<br />

<div align="center">
  <a href="https://github.com/OPPOMKLab/u-LLaVA">
    <img src="./images/logo.png" alt="Logo" width="80" height="80">
  </a>

<h3 align="center">u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model</h3>

  <p align="center">
    å¤šæ¨¡æ€å¤šä»»åŠ¡LLM
    <br />
    <a href="https://github.com/OPPOMKLab/u-LLaVA/blob/main/README.md"><strong> Documentation</strong></a>
      |
    <a href="https://github.com/OPPOMKLab/u-LLaVA/blob/main/README_zh.md"><strong> ä¸­æ–‡æ–‡æ¡£ </strong></a>
    <br />
    <br />
    <a href="https://arxiv.org/abs/2311.05348">è®ºæ–‡</a>
    Â·
    <a href="https://github.com/OPPOMKLab/u-LLaVA/issues">åé¦ˆBug</a>
    Â·
    <a href="https://github.com/OPPOMKLab/u-LLaVA/issues">æ–°ç‰¹æ€§</a>
  </p>

</div>

## ğŸ‰ News

- **\[2024/07\]** æˆ‘ä»¬å°†å¾ˆå¿«æ›´æ–°[grounding&segmentationç‰ˆullavaæƒé‡][].

- **\[2024/07\]** 336åˆ†è¾¨ç‡æ”¯æŒï¼ŒMM-Bench, TextVQA, SQA, GQA ç»“æœæ›´æ–°.

- **\[2024/07\]** [Salient-15k][salient_15k] æ ‡æ³¨æ–‡ä»¶å·²æ›´æ–°.

- **\[2024/07\]** æœ¬å·¥ä½œå·²è¢«**ECAI 2024**æ¥æ”¶ï¼Œæ„Ÿè°¢æ‰€æœ‰å‰åºå·¥ä½œï¼

- **\[2024/01\]** [ä»£ç ][https://github.com/OPPOMKLab/u-LLaVA/] and [åˆ†å‰²ç‰ˆæœ¬æƒé‡][ullava] å·²å¼€æº.

- **\[2023/10\]** [è®ºæ–‡][https://arxiv.org/abs/2311.05348] å·²æäº¤.

  

<!-- TABLE OF CONTENTS -->

<details>
  <summary>ç›®å½•</summary>
  <ol>
    <li>
      <a href="#about-the-project">å…³äºé¡¹ç›®</a>
      <ul>
        <li><a href="#features">ç‰¹è‰²</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">å¼€å§‹</a>
      <ul>
        <li><a href="#requirements">é…ç½®è¦æ±‚</a></li>
        <li><a href="#datasets">æ•°æ®é›†</a></li>
        <li><a href="#training">è®­ç»ƒ</a></li>
        <li><a href="#evaluation">æµ‹è¯•</a></li>
      </ul>
    </li>
    <li><a href="#license">License</a></li>
    <li><a href="#citation">å¼•ç”¨</a></li>
    <li><a href="#acknowledgments">è‡´è°¢</a></li>
  </ol>
</details>




<!-- ABOUT THE PROJECT -->

## å…³äºé¡¹ç›®
æ¨¡å‹ç»“æ„:

<div align="center">
    <img src=./images/llm.png width=70%>
</div>


æ ·ä¾‹ï¼š
<div align="center">
    <img src=./images/exp1.png width=70%>
</div>

<div align="center">
 <img src=./images/exp2.png width=70%>
</div>

<div align="center">
 <img src=./images/exp3.png width=70%>
</div>



<p align="right">(<a href="#readme-top">back to top</a>)</p>

Demoå³å°†ä¸Šçº¿ã€‚

<!-- Features -->

## ç‰¹è‰²

**ä»£ç **

- [x] è®­ç»ƒEpoché‡åŒ–æµ‹è¯•

  - [x] è‡ªå®šä¹‰Compute metricsï¼Œé€‚é…transformers

- [x] æ··åˆæ•°æ®é›†

  - [x] æ•°æ®é›†æ¯”ä¾‹æŒ‡å®š
  - [x] æ–‡æœ¬æ•°æ®é›†ã€å›¾æ–‡æ•°æ®é›†ã€è§†é¢‘æ–‡æ•°æ®é›†

- [x] DeepSpeed

- [x] LoRA

  

**ä»»åŠ¡**

- [x] è§†è§‰ç†è§£
  - [x] å›¾Captioning
  - [x] è§†é¢‘Captioning
  - [x] è§†è§‰é—®ç­” (VQA)
- [x] åˆ†å‰²
  - [x] æŒ‡ä»£åˆ†å‰² (RES)
  - [x] æ˜¾è‘—æ€§ç›®æ ‡åˆ†å‰²
  - [x] è¯­ä¹‰åˆ†å‰²
- [x] è§†è§‰Grounding
    - [x] æŒ‡ä»£ç†è§£ (REC)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- Models -->

## æ¨¡å‹å¼€æº

| Models  |            Images/Videos |
| :------ | -----------------------: |
| u-LLaVA | [uLLaVA Stage 2][ullava] |

<!-- GETTING STARTED -->

## å¼€å§‹

<!-- Requirements -->

### é…ç½®è¦æ±‚

ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤:
```shell
pip install -r ./shells/requirements.txt
cd ./models/GroundingDINO && ./install.sh && cd ../..
```
æŒ‡ä»¤æ„ä¹‰ï¼š
1. å®‰è£…ullavaæ‰€éœ€åº“: `pip install -r requirements.txt`
2. æ„å»ºGroundingDINO cudaä¾èµ–åº“: `cd ./models/GroundingDINO && ./install.sh && cd ../..`, 
    å¦‚æœä¹‹å‰æœªé…ç½®ï¼Œå¯èƒ½ä¼šå‡ºç°ä»¥ä¸‹å‘Šè­¦ `UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!
    warnings.warn("Failed to load custom C++ ops. Running on CPU mode Only!")`
3. å¦‚æœGroundingDINOå‡ºç°é—®é¢˜ï¼Œå¯ä»¥å…³é—­æ‰ä»£ç ä¸­æ‰€æœ‰GroundingDINOç›¸å…³æ¨¡å—ï¼Œä»¥çº¯åˆ†å‰²å½¢å¼è¿è¡Œ

<!-- Datasets -->

## æ•°æ®é›†

æˆ‘ä»¬å¯¹ä½¿ç”¨åˆ°çš„æ•°æ®é›†çš„æ ‡æ³¨æ–‡ä»¶è¿›è¡Œäº†é‡æ„ï¼Œæ–¹ä¾¿è®­ç»ƒå’Œç†è§£ï¼Œè¯·ä¸‹è½½æˆ‘ä»¬é‡æ„åçš„æ ‡æ³¨æ–‡ä»¶ã€‚

**ä¸‹è½½é“¾æ¥**: [ullava modified annotations][ullava_database], [LLaVA pretrain annotations][llava_cc3m_anno] and [LLaVA finetuning annotaions][llava_instruct_150k]

**è®­ç»ƒå›¾åƒå­˜å‚¨ç¤ºä¾‹** (åè¡¨ä¸­æœ‰å›¾åƒæ–‡ä»¶ä¸‹è½½é“¾æ¥):

```
image_root
â”œâ”€ade20k
â”‚  â”œâ”€annotations
â”‚  â””â”€images
â”œâ”€coco2014
â”‚  â”œâ”€test2014
â”‚  â”œâ”€train2014
â”‚  â””â”€val2014
â”œâ”€coco2017
â”‚  â”œâ”€annotations
â”‚  â”œâ”€train2017
â”‚  â””â”€val2017
â”œâ”€cocostuff
â”‚  â”œâ”€train2017
â”‚  â””â”€val2017
â”œâ”€LLaVA-CC3M-Pretrain-595K
â”‚  â””â”€images
â”œâ”€saiapr_tc-12
â”‚  â”œâ”€00
â”‚  â””â”€01
â””â”€vlpart
    â”œâ”€paco
    â”‚  â””â”€annotations
    â””â”€pascal-part
        â”œâ”€Annotations_Part
        â”œâ”€examples
        â””â”€VOCdevkit
```

å…¶ä¸­ ade20k ç”± ADEChallengeData2016.zip è§£å‹å¹¶é‡å‘½åï¼Œcocostuffç”± stuffthingmaps_trainval2017.zipè§£å‹å¹¶é‡å‘½åã€‚

### Stage I: é¢„è®­ç»ƒ
| Dataset | Images/Videos | Annotations |
| :-----| ----: | :----: |
| LLaVA CC3M | [LLaVA-CC3M-Pretrain-595K/image.zip][llava_cc3m_image] | [chat.json][llava_cc3m_anno] |
| TGIF | [TGIF - Quark Drive ][tgif_quark] | [tgif.json][ullava_database] |

è¯·æ³¨æ„ï¼šæˆ‘ä»¬å¯¹TGIFæ•°æ®é›†è¿›è¡Œäº†é‡å‘½åå¹¶å‰”é™¤äº†æ— æ•ˆæ ·æœ¬ï¼Œä»¥æ–¹ä¾¿è®­ç»ƒï¼Œä½†è¯·å¤§å®¶éµå¾ªåŸå§‹TGIFæ•°æ®é›†çš„LICENSEã€‚

### Stage II: å¾®è°ƒ
| Dataset | Images | Annotations |
| :-----| ----: | :----: |
| LLaVA Instruction 150K | [coco2017][coco2014_images] | [llava_instruct_150k.json][llava_instruct_150k] |
| RefCOCO | [coco2014][coco2014_images] | [refcoco_train.json][ullava_database] |
| RefCOCOg | [coco2014][coco2014_images] | [refcocog_train.json][ullava_database] |
| RefCOCO+ | [coco2014][coco2014_images] | [refcoco+_train.json][ullava_database] |
| RefCLEF | [saiapr_tc-12][saiapr_tc-12] | [refclef_train.json][ullava_database] |
| ADE20K | [ade20k][ade20k] | [ade20k.json][ullava_database] |
| COCO Stuff | [cocostuff][coco_stuff] | [cocostuff.json][ullava_database]  |
| VOC2010 | [voc2010][voc2010] | [pascal_part.json][ullava_database] |
| PACO LVIS  | [paco][paco] | [paco_lvis.json][ullava_database] |
| Salient 15K | coming soon | coming soon |

æ•°æ®é›†é…ç½®ç¤ºä¾‹

```yaml
dataset:
  llava:
    data_type: 'image'
    image_token_len: 256
    build_info:
      anno_dir: '/path_to_annotations/llava_instruct_150k.json'
      image_dir: '/path_to_image_root/coco2017/train2017'
      portion: 1.0
    vis_processor: 'clip_image'

  refcoco+:
    data_type: 'image'
    image_token_len: 256
    build_info:
      anno_dir: '/path_to_annotations/refcoco+_train.json'
      image_dir: '/path_to_image_root/coco2014'
      template_root: './datasets/templates/SEG.json'
      portion: 1.0
    vis_processor: 'clip_image'
```

<!-- Training -->

## è®­ç»ƒ

### Stage I: é¢„è®­ç»ƒ

1. å‡†å¤‡å¼€æºæ¨¡å‹

| Foundation model | Version | Path |
| :-----| ----: | :----: |
| Vicuna 7B HF | V1.1 | [vicuna_7b_v1.1][vicuna_7b_v1.1] |
| LLaMA2 7B HF | - | [meta-llama/Llama-2-7b-hf][llama2_7b] |
| SAM | ViT-H | [sam_vit_h_4b8939.pth][sam_vit_h] |
| GroundingDINO | swint_ogc | [groundingdino_swint_ogc.pth][groundingdino_swint_ogc] |

*Note:*

*- LLaMA2 ç”± `bf16`è®­ç»ƒ, å¦‚æœä»¥ `fp16`è¿›è¡Œä¸€é˜¶æ®µè®­ç»ƒæ—¶ï¼Œå¯èƒ½å‡ºç°æ”¶æ•›é”™è¯¯.*

*- LLaMA2 é»˜è®¤çš„ `tokenizer.legacy` ä¸º False, å› æ­¤ä½¿ç”¨æŸäº› conversation æ¨¡æ¿æ—¶å¯èƒ½å‡ºç°ç¼–è§£ç é”™è¯¯.* 

*- æ›´æ­£: è®ºæ–‡ä¸­ä½¿ç”¨çš„åŸºæ¨¡å‹ä¸º `Vicuna-v1.1`, è€Œä¸æ˜¯LLaMA2ï¼Œéå¸¸æŠ±æ­‰å‡ºç°äº†ç¬”è¯¯.*


2. å‡†å¤‡æ•°æ®é›†
3. è®¾ç½®é…ç½®æ–‡ä»¶
```text
configs/train/ullava_core_stage1.yaml
```
è¯·æ³¨æ„é…ç½®å¥½æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ¨¡å‹è·¯å¾„.
4. å¤šGPUè®­ç»ƒStage I
```shell
./shells/pretrain.sh
```
æˆ–è€…å• GPU `python train_ullava_core.py --cfg_path './configs/train/ullava_core_stage1.yaml'` .

ç¬¬ä¸€é˜¶æ®µä½¿ç”¨ 4 ä¸ª A100 80G å’Œ bf16ï¼Œ1 ä¸ªå‘¨æœŸèŠ±è´¹çº¦ 6 å°æ—¶ã€‚ ç„¶åä½ å¯ä»¥åœ¨output_diræ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œ
ä¾‹å¦‚ï¼Œâ€œ./exp/ullava_core_7bâ€

### Stage II: å¾®è°ƒ

Stage I å®Œæˆä¹‹åï¼Œå³å¯ä»¥è¿›è¡Œä¸‹ä¸€é˜¶æ®µçš„è®­ç»ƒï¼Œ
1. å‡†å¤‡æ•°æ®é›†
2. è®¾ç½®é…ç½®æ–‡ä»¶
```text
configs/train/ullava_stage2_lora.yaml (for lora)
configs/train/ullava_stage2.yaml (for non lora)
```
3. å¤šGPUè®­ç»ƒ
```shell
./shells/finetune.sh
```
æˆ–è€…å•GPU LoRAå¾®è°ƒï¼špython train_ullava.py --cfg_path './configs/train/ullava_stage2_lora.yaml'` .


### å¸¸è§é—®é¢˜
Q1: ä½¿ç”¨äº†å“ªç§conversation æ¨¡æ¿?

A1: Stage I: 'conv_simple'. Stage II: 'conv_sep2'

Q2: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨LoRA?

A2: Stage I: æˆ‘ä»¬æœªä½¿ç”¨. Stage II: æ ¹æ®æ‚¨çš„è®¾å¤‡.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- Evaluation -->

## æµ‹è¯•

### æ‰¹é‡é‡åŒ–æµ‹è¯•

1. é…ç½®æ–‡ä»¶
```text
configs/eval/eval_res.ymal (for RES task)
configs/eval/eval_rec.ymal (for REC task)
configs/eval/eval_salient.ymal (for Salinet segmentation task)
```
2. è¿è¡Œ
```text
python evaluation/eval_ullava.py --cfg_path './configs/eval/eval_res.yaml' (for RES)
python evaluation/eval_ullava_grounding.py --cfg_path './configs/eval/eval_rec.yaml' (for REC)
python evaluation/eval_ullava.py --cfg_path './configs/eval/eval_salient.yaml' (for Salinet)
```



<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- Qualitative Evaluation -->

### å®šæ€§æµ‹è¯•

è°ƒæ•´ `evaluation/inference_ullava_core.py` å’Œ`evaluation/inference_ullava.py` çš„argparseré…ç½®ï¼Œè¿›è¡Œä¸€é˜¶æ®µå’ŒäºŒé˜¶æ®µçš„å®šæ€§æµ‹è¯•

```text
python evaluation/eval_ullava.py
python evaluation/eval_ullava_grounding.py 
```

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- LICENSE -->

## License

Distributed under the Apache License. See `LICENSE` for more information.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- Citation -->

## å¼•ç”¨

```
@inproceedings{xu2024ullava,
  title={u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model},
  author={Xu, Jinjin and Xu, Liwu and Yang, Yuzhe and Li, Xiang and Wang, Fanyi and Xie, Yanchun and Huang, Yi-Jie and Li, Yaqian},
  booktitle={Proceedings of the 27th European Conference on Artificial Intelligence},
  year={2024}
}
```

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- TODO -->

## å¾…åŠ

- [ ] Visual Segmentation
  - [ ] Instance Segmentation

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- ACKNOWLEDGMENTS -->
## è‡´è°¢
ç”±è¡·æ„Ÿè°¢ä»¥ä¸‹å¼€æºå·¥ä½œçš„è´¡çŒ®ï¼Œä¸”æœ¬å·¥ä½œç”±ä¸Šæµ·å¸‚ç™½ç‰å…°æµ¦æ±Ÿäººæ‰è®¡åˆ’æ”¯æŒ (é¡¹ç›®ç¼–å·ï¼š23PJ1421800)ã€‚

* [LLaVA](https://github.com/haotian-liu/LLaVA)
* [LISA](https://github.com/dvlab-research/LISA)
* [VideoLLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA)
* [Shikra](https://github.com/shikras/shikra)
* [SAM](https://github.com/facebookresearch/segment-anything)
* [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)

<p align="right">(<a href="#readme-top">back to top</a>)</p>




See the [open issues](https://github.com/OPPOMKLab/u-LLaVA/issues) for a full list of proposed features (and known issues).

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->

[llava_cc3m_image]: https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/images.zip
[llava_cc3m_anno]: https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/chat.json
[llava_instruct_150k]: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K
[coco2014_images]: https://cocodataset.org/#download
[coco2017_images]: https://cocodataset.org/#download
[ullava_database]: https://huggingface.co/datasets/jinxu95/ullava/tree/main
[saiapr_tc-12]: https://web.archive.org/web/20220515000000/http://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip
[ade20k]: http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip
[coco_stuff]: http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip
[voc2010]: http://host.robots.ox.ac.uk/pascal/VOC/voc2010/VOCtrainval_03-May-2010.tar
[paco]: https://dl.fbaipublicfiles.com/paco/annotations/paco_lvis_v1.zip
[tgif_quark]: https://pan.quark.cn/s/4440590bceed
[llama2_7b]: https://huggingface.co/meta-llama/Llama-2-7b-hf
[vicuna_7b_v1.1]: https://huggingface.co/lmsys/vicuna-7b-v1.1
[sam_vit_h]: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
[groundingdino_swint_ogc]: https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
[ullava]: https://huggingface.co/jinxu95/ullava
