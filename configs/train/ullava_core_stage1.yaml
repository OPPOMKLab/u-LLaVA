model:
  arch: 'ullava_core'
  llm_path: './model_zoo/vicuna_7b_v1.1'
  vision_encoder: './model_zoo/clip-vit-large-patch14'
  vision_hidden_layer: -2
  projector_from_scratch: true
  conv_type: 'conv_simple'

task:
  type: image_text_pretrain
  collator_type: 'image_video_collator'

processor:
  clip_image:
    path: './model_zoo/clip-vit-large-patch14'
    image_size: 224
  gif_train:
    n_frm: 8
    image_size: 224

dataset:
  llava_cc3m:
    data_type: 'image'
    image_token_len: 256
    build_info:
      anno_dir: './dataset_zoo/LLaVA-CC3M-Pretrain-595K/chat.json'
      image_dir: './dataset_zoo/LLaVA-CC3M-Pretrain-595K/images'
      portion: 1.0
    vis_processor: 'clip_image'

  tgif:
    data_type: 'gif'
    image_token_len: 256
    build_info:
      anno_dir: './dataset_zoo/TGIF/tgif_llava.json'
      image_dir: './dataset_zoo/TGIF/GIFs'
      portion: 1.0
    vis_processor: 'gif_train'

training:
  deepspeed: './configs/deepspeed/bf16_zero2.json'
  output_dir: './exp/ullava/ullava_stage1'
  learning_rate: 2e-3
  remove_unused_columns: false
  model_max_length: 1024
  disable_tqdm: false
  fp16: false
  bf16: true
  tf32: false
  per_device_train_batch_size: 48
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 1
  num_train_epochs: 1
  evaluation_strategy: 'no'
  save_strategy: 'steps'
  save_steps: 5000
  save_total_limit: 1
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: 'cosine'
  logging_steps: 1
  dataloader_num_workers: 8
  gradient_checkpointing: true
  seed: 42
